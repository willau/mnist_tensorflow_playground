{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST number classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "x_train = mnist.train.images\n",
    "y_train = mnist.train.labels\n",
    "x_test = mnist.test.images\n",
    "y_test = mnist.test.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1253c12b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFOEHgiL8QFCEHwiK8ANB/R81jmCIYi6NiQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x122685390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_train = np.shape(x_train)[0]\n",
    "idx = np.random.randint(n_train)\n",
    "vector = x_train[idx]\n",
    "img = np.reshape(vector, (28, 28))\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow and TensorBoard helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_summaries(var, scope='summaries'):\n",
    "    with tf.name_scope(scope):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "        \n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name='W')\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial, name='b')\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], \n",
    "                        padding='SAME', name='convolution')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], \n",
    "                          padding='SAME', name='max_pool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One CNN to rule them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset graph (important for batch normalization and summary)\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# placeholder\n",
    "x_input = tf.placeholder(tf.float32, [None, 784])\n",
    "y_input = tf.placeholder(tf.float32, [None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    \n",
    "    with tf.name_scope(\"reshape\"):\n",
    "        x_image = tf.reshape(x_input, [-1, 28, 28, 1])\n",
    "        tf.summary.image('input', x_image, 3)\n",
    "        \n",
    "    with tf.name_scope(\"conv1\"):\n",
    "        W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "        b_conv1 = bias_variable([32])\n",
    "        h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "        h_pool1 = max_pool_2x2(h_conv1)\n",
    "        variable_summaries(W_conv1, \"weights\")\n",
    "        variable_summaries(b_conv1, \"biases\")\n",
    "        \n",
    "    with tf.name_scope(\"conv2\"):\n",
    "        W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "        b_conv2 = bias_variable([64])\n",
    "        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "        h_pool2 = max_pool_2x2(h_conv2)\n",
    "        variable_summaries(W_conv2, \"weights\")\n",
    "        variable_summaries(b_conv2, \"biases\")\n",
    "            \n",
    "    with tf.name_scope(\"fully_connected\"):\n",
    "        W = weight_variable([7 * 7 * 64, 1024])\n",
    "        b = bias_variable([1024])\n",
    "        h_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n",
    "        h_relu = tf.nn.relu(tf.matmul(h_flat, W) + b)\n",
    "        \n",
    "    with tf.name_scope(\"dropout\"):\n",
    "        h_drop = tf.nn.dropout(h_relu, keep_prob) \n",
    "        \n",
    "    with tf.name_scope(\"readout\"):\n",
    "        W2 = weight_variable([1024, 10])\n",
    "        b2 = bias_variable([10])\n",
    "        y_conv = tf.matmul(h_drop, W2) + b2\n",
    "        \n",
    "    with tf.name_scope(\"cross_entropy\"):\n",
    "        cross_entropy = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=y_input, \n",
    "                logits=y_conv))\n",
    "        tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "        \n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        train_step = tf.train.AdamOptimizer().minimize(cross_entropy)\n",
    "        \n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_prediction = tf.equal(tf.argmax(y_conv, axis=1), tf.argmax(y_input, axis=1)) \n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "        \n",
    "    merged = tf.summary.merge_all()\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test graph architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "CPU times: user 22.4 s, sys: 6.86 s, total: 29.2 s\n",
      "Wall time: 20.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_size = 5000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    idx = np.random.randint(0, 10000, size=(test_size,))\n",
    "    xs, ys = mnist.test.images[idx], mnist.test.labels[idx]\n",
    "    feed_dict = {x_input: xs, y_input: ys, keep_prob: 1}\n",
    "    __ = sess.run(merged, feed_dict=feed_dict)\n",
    "    print(np.shape(__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean summaries folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_folder(path=\"./tensorflow_summaries/\"):\n",
    "    for file in os.listdir(path):\n",
    "        filepath = path + file\n",
    "        if os.path.isfile(filepath):\n",
    "            os.remove(filepath)\n",
    "            \n",
    "clean_folder(path=\"./tensorflow_summaries/train/\")\n",
    "clean_folder(path=\"./tensorflow_summaries/test/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model [delete random sampling later]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc at epoch 0: 0.151\n",
      "Acc at epoch 100: 0.947\n",
      "Acc at epoch 200: 0.9578\n",
      "Acc at epoch 300: 0.9638\n",
      "Acc at epoch 400: 0.9774\n",
      "Acc at epoch 500: 0.9786\n",
      "Acc at epoch 600: 0.9754\n",
      "Acc at epoch 700: 0.9818\n",
      "Acc at epoch 800: 0.9788\n",
      "Acc at epoch 900: 0.982\n",
      "Acc. on test: 0.984800\n",
      "CPU times: user 15min 37s, sys: 1min 57s, total: 17min 34s\n",
      "Wall time: 8min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# feed dict helper\n",
    "def feed_dict(train=True, dropout=1):\n",
    "    batch_size = 50\n",
    "    if train:\n",
    "        xs, ys = mnist.train.next_batch(batch_size)\n",
    "        k = dropout\n",
    "    else:\n",
    "        # random sampling\n",
    "        test_size=5000\n",
    "        idx = np.random.randint(0, 10000, size=test_size)\n",
    "        xs, ys = mnist.test.images[idx], mnist.test.labels[idx]\n",
    "        k = 1\n",
    "    return {x_input: xs, y_input: ys, keep_prob: dropout}\n",
    "\n",
    "# dropout: keep probability\n",
    "k = 0.5\n",
    "\n",
    "# using graph for training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    train_writer = tf.summary.FileWriter(\"./tensorflow_summaries/train\", sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(\"./tensorflow_summaries/test\")\n",
    "    \n",
    "    for i in range(1000):\n",
    "        if i % 100 == 0:\n",
    "            summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))\n",
    "            test_writer.add_summary(summary, i)\n",
    "            print('Acc at epoch %s: %s' % (i, acc))\n",
    "        else:\n",
    "            summary, __ = sess.run([merged, train_step], feed_dict=feed_dict(True, k))\n",
    "            train_writer.add_summary(summary, i)\n",
    "        \n",
    "    # test\n",
    "    out = sess.run(accuracy, feed_dict=feed_dict(False))\n",
    "    print(\"Acc. on test: %f\" % out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
