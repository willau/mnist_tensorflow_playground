{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "x_train = mnist.train.images\n",
    "y_train = mnist.train.labels\n",
    "x_test = mnist.test.images\n",
    "y_test = mnist.test.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xf548be0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkBThB5Ii/EBShB9IivADSRF+IKn/AzD+V4nKebt+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xf3bbe80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_train = np.shape(x_train)[0]\n",
    "idx = np.random.randint(n_train)\n",
    "vector = x_train[idx]\n",
    "img = np.reshape(vector, (28, 28))\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow and TensorBoard helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_folder(path=\"tensorflow_summaries/\"):\n",
    "    for file in os.listdir(path):\n",
    "        filepath = path + file\n",
    "        if filepath[-1] != \"/\":\n",
    "            filepath += \"/\"\n",
    "        if os.path.isfile(filepath):\n",
    "            os.remove(filepath)\n",
    "        \n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "        \n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One CNN to rule them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reset graph (important for batch normalization and summary)\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# placeholder\n",
    "x_input = tf.placeholder(tf.float32, [None, 784])\n",
    "y_input = tf.placeholder(tf.float32, [None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    \n",
    "    with tf.name_scope(\"reshape\"):\n",
    "        x_image = tf.reshape(x_input, [-1, 28, 28, 1])\n",
    "        \n",
    "    with tf.name_scope(\"1st_convolution\"):\n",
    "        W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "        b_conv1 = bias_variable([32])\n",
    "        h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "        h_pool1 = max_pool_2x2(h_conv1)\n",
    "        variable_summaries(W_conv1)\n",
    "        variable_summaries(b_conv1)\n",
    "        \n",
    "    with tf.name_scope(\"2nd_convolution\"):\n",
    "        W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "        b_conv2 = bias_variable([64])\n",
    "        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "        h_pool2 = max_pool_2x2(h_conv2)\n",
    "        variable_summaries(W_conv2)\n",
    "        variable_summaries(b_conv2)\n",
    "            \n",
    "    with tf.name_scope(\"fully_connected\"):\n",
    "        W = weight_variable([7 * 7 * 64, 1024])\n",
    "        b = bias_variable([1024])\n",
    "        h_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n",
    "        h_relu = tf.nn.relu(tf.matmul(h_flat, W) + b)\n",
    "        \n",
    "    with tf.name_scope(\"dropout\"):\n",
    "        h_drop = tf.nn.dropout(h_relu, keep_prob) \n",
    "        \n",
    "    with tf.name_scope(\"readout\"):\n",
    "        W2 = weight_variable([1024, 10])\n",
    "        b2 = bias_variable([10])\n",
    "        y_conv = tf.matmul(h_drop, W2) + b2\n",
    "        \n",
    "    with tf.name_scope(\"cross_entropy\"):\n",
    "        cross_entropy = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=y_input, \n",
    "                logits=y_conv))\n",
    "        tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "        \n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        train_step = tf.train.AdamOptimizer().minimize(cross_entropy)\n",
    "        \n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_prediction = tf.equal(tf.argmax(y_conv, axis=1), tf.argmax(y_input, axis=1)) \n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "        \n",
    "    merged = tf.summary.merge_all()\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test graph architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "Wall time: 600 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    xs, ys = mnist.train.next_batch(50)\n",
    "    feed_dict = {x_input: xs, y_input: ys, keep_prob: 1}\n",
    "    __ = sess.run(merged, feed_dict=feed_dict)\n",
    "    print(np.shape(__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean summaries folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_folder(path=\"./tensorflow_summaries/train/\")\n",
    "clean_folder(path=\"./tensorflow_summaries/test/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model [delete random sampling]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc at epoch 0: 0.12\n",
      "Acc at epoch 10: 0.42\n",
      "Acc at epoch 20: 0.62\n",
      "Acc at epoch 30: 0.76\n",
      "Acc at epoch 40: 0.92\n",
      "Acc at epoch 50: 0.84\n",
      "Acc at epoch 60: 0.88\n",
      "Acc at epoch 70: 0.92\n",
      "Acc at epoch 80: 0.92\n",
      "Acc at epoch 90: 0.96\n",
      "Acc. on test: 0.920000\n"
     ]
    }
   ],
   "source": [
    "# feed dict helper\n",
    "def feed_dict(train=True, dropout=1):\n",
    "    batch_size = 50\n",
    "    if train:\n",
    "        xs, ys = mnist.train.next_batch(batch_size)\n",
    "        k = dropout\n",
    "    else:\n",
    "        # random sampling\n",
    "        idx = np.random.randint(0, 10000, size=(batch_size,))\n",
    "        xs, ys = mnist.test.images[idx], mnist.test.labels[idx]\n",
    "        k = 1\n",
    "    return {x_input: xs, y_input: ys, keep_prob: dropout}\n",
    "\n",
    "# dropout: keep probability\n",
    "k = 0.5\n",
    "\n",
    "# using graph for training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    train_writer = tf.summary.FileWriter(\"./tensorflow_summaries/train\", sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(\"./tensorflow_summaries/test\")\n",
    "    \n",
    "    for i in range(100):\n",
    "        if i % 10 == 0:\n",
    "            summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))\n",
    "            test_writer.add_summary(summary, i)\n",
    "            print('Acc at epoch %s: %s' % (i, acc))\n",
    "        else:\n",
    "            summary, __ = sess.run([merged, train_step], feed_dict=feed_dict(True, k))\n",
    "            train_writer.add_summary(summary, i)\n",
    "        \n",
    "    # test\n",
    "    out = sess.run(accuracy, feed_dict=feed_dict(False))\n",
    "    print(\"Acc. on test: %f\" % out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
